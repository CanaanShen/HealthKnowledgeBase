package knowledge.project.testing;

import java.io.StringReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;
import java.util.Properties;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import edu.stanford.nlp.ie.crf.CRFClassifier;
import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.parser.lexparser.LexicalizedParser;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.process.CoreLabelTokenFactory;
import edu.stanford.nlp.process.PTBTokenizer;
import edu.stanford.nlp.process.TokenizerFactory;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import edu.stanford.nlp.trees.GrammaticalStructure;
import edu.stanford.nlp.trees.GrammaticalStructureFactory;
import edu.stanford.nlp.trees.PennTreebankLanguagePack;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreebankLanguagePack;
import edu.stanford.nlp.trees.TypedDependency;
import edu.stanford.nlp.trees.international.pennchinese.ChineseTreebankLanguagePack;
import edu.stanford.nlp.util.CoreMap;

public class ParsingTester {
	
	private void testParsing() {
//		Properties props = new Properties();
//		props.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner, parse");
//		StanfordCoreNLP pipeLine = new StanfordCoreNLP(props);
//		
		String doc = "çŒªè‚‰ çš? è›‹ç™½è´? å±? ä¼˜è´¨ è›‹ç™½è´? ï¼? å«æœ‰ äººä½“ å…¨éƒ¨ å¿…éœ€ æ°¨åŸºé…?";
		List<String> docList = new ArrayList<String>();
		docList.add("çŒªè‚‰");
		docList.add("è›‹ç™½è´?");
		docList.add("äººä½“"); 
		docList.add("å…¨éƒ¨");
//		
//		Annotation document = new Annotation(doc);
//		pipeLine.annotate(document);
		
	    //LexicalizedParser lp = LexicalizedParser.loadModel("edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz");
		
        MaxentTagger tagger = new MaxentTagger(
                "edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger");
	    String docTag = tagger.tagTokenizedString(doc);
	    System.out.println(docTag);
	    
	    /*
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		for(CoreMap sentence: sentences) {
			System.out.println(sentence);
		
			List<CoreLabel> tokenList = sentence.get(TokensAnnotation.class);
			for(CoreLabel token: tokenList) {
				String lemma = token.getString(LemmaAnnotation.class);
				String pos = token.getString(PartOfSpeechAnnotation.class);
				String ner = token.getString(NamedEntityTagAnnotation.class);
				System.out.println("lemma: " + lemma + " pos: " + pos + " ner " + ner);
			}
			
		    Tree parse = lp.apply(tokenList);
		    parse.pennPrint();
		}//for
		*/
	}
	
	//test segment
	private void testSegment() {
		
		String baseDir = "edu/stanford/nlp/models/segmenter/chinese/";
		Properties property = new Properties();
	    property.setProperty("sighanCorporaDict", baseDir);

		property.setProperty("serDictionary", baseDir + "dict-chris6.ser.gz");
		property.setProperty("inputEncoding", "utf-8");
		property.setProperty("sighanPostProcessing", "true");
		
		CRFClassifier<CoreLabel> segmenter = new CRFClassifier<CoreLabel>(property);
		segmenter.loadClassifierNoExceptions(baseDir + "ctb.gz", property);
		segmenter.flags.setProperties(property);
		
		String sentence = "çŒªè‚‰çš„è›‹ç™½è´¨å±ä¼˜è´¨è›‹ç™½è´¨ï¼Œå«æœ‰äººä½“å…¨éƒ¨å¿…éœ?æ°¨åŸºé…?";
		String[] strArray = (String[]) segmenter.segmentString(sentence).toArray();
		for(String str: strArray) {
			System.out.println(str);
		}
	}
	
	private void parseTesting() {
		String baseDir = "edu/stanford/nlp/models/";
		LexicalizedParser lexicalParser = LexicalizedParser.loadModel(baseDir + "lexparser/xinhuaFactored.ser.gz");
		String sent = "çŒªè‚‰  å«æœ‰  ä¸°å¯Œ  çš?  è›‹ç™½è´?  å’?  è„‚è‚ªé…?  ï¼? å…·æœ‰   å¾ˆå¤š  ä½œç”¨ ã€‚çŒªè‚?   å‘³é“  æ¸…æ·¡  ã€?";
		
	     TokenizerFactory<CoreLabel> tokenizerFactory =
	             PTBTokenizer.factory(new CoreLabelTokenFactory(), "");
	     List<CoreLabel> rawSentence =
	    	      tokenizerFactory.getTokenizer(new StringReader(sent)).tokenize();
		Tree tree = (Tree) lexicalParser.apply(rawSentence);
		List<TaggedWord> taggedWordList = tree.taggedYield();
		for(TaggedWord taggedWord: taggedWordList) {
			//System.out.println(taggedWord.word() + " : " + taggedWord.tag());
		}
		
		TreebankLanguagePack tlp = new ChineseTreebankLanguagePack();
		GrammaticalStructureFactory gramStrtFactory = tlp.grammaticalStructureFactory();
		GrammaticalStructure gramStrt = gramStrtFactory.newGrammaticalStructure(tree);
	    
		Collection<TypedDependency> tdl = null;
		tdl = gramStrt.typedDependenciesCCprocessed();
		for(TypedDependency type: tdl) {
			System.out.print(type.reln().getShortName());
			//System.out.print(" gov.originalText: " + type.gov().originalText() );
			System.out.println("   : " + type.dep().word() + " ; " + type.dep().originalText());
			System.out.println();
//			System.out.println(type.reln()());
		}

	}
	
	public static void main(String[] args) {
		
		ParsingTester tester = new ParsingTester();
		//tester.testParsing();
//		tester.testSegment();
//		String str = "I am a student.";
//		tester.parseTesting();
//		String str = "(loc, æ¶ˆé™¤#VV, é£?#NN)  (case, é£?#NN, å?#LC)  (mmod, æ¶ˆé™¤#VV, å?#VV)  (root, ROOT#null, æ¶ˆé™¤#VV)  (assmod, é¥¥é¥¿æ„?#NN, è‚¥èƒ–äº?#NN) ";
//		Pattern pattern = Pattern.compile("\\(.+\\)");
//		String[] split = pattern.split("\\(.*\\)");
//		for(String splitStr: split) {
////			System.out.println(splitStr);
//		}
		//		Matcher m = pattern.matcher(str);
//		System.out.println(m.groupCount());
//		if(m.matches()) {
//			System.out.println(m.group());
//		}
		String str = " 1. è¥¿ç“œæ¸…çƒ­è§£æ¯’";
		str = str.replaceFirst("\\d.", "");
		System.out.println(str);
		System.out.println("Program ends");
	}
	
}
